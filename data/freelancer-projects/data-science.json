[
  {
    "title": "Müşteri Davranışı Tahmin Modeli",
    "description": "<p>E-ticaret sitemizdeki satışları artırmak için, hangi müşterinin önümüzdeki ay bizi terk edeceğini (Churn Prediction) ve hangisinin tekrar satın alma yapacağını öngören bir Makine Öğrenmesi modeli istiyoruz. Verilerimiz SQL veritabanında ham halde duruyor.</p><p><strong>Proje adımları:</strong></p><ul><li><strong>Veri Hazırlığı:</strong> Müşterilerin son 2 yıldaki sipariş geçmişi, site içi gezinme logları ve demografik bilgileri birleştirilerek temizlenmeli ve eksik veriler (Missing Values) uygun yöntemlerle doldurulmalıdır.</li><li><strong>Özellik Mühendisliği (Feature Engineering):</strong> 'Son satın alma üzerinden geçen gün', 'Ortalama sepet tutarı', 'İade oranı' gibi modeli güçlendirecek yeni öznitelikler türetilmelidir.</li><li><strong>Model Seçimi:</strong> XGBoost, Random Forest ve LightGBM algoritmaları denenmeli; Accuracy, Precision ve Recall metriklerine göre en başarılı model seçilmelidir.</li><li><strong>Hiperparametre Optimizasyonu:</strong> Seçilen modelin başarısını maksimize etmek için Grid Search veya Bayesian Optimization yöntemleri ile parametre ayarları yapılmalıdır.</li><li><strong>Sunum:</strong> Modelin çıktılarını pazarlama ekibinin anlayacağı dilde (Örn: 'Bu müşteriye %20 indirim kuponu atarsak kalma ihtimali %80 artar') raporlayan bir sunum hazırlanmalıdır.</li></ul><p>Kodların Jupyter Notebook formatında, her adımı açıklamalı (Markdown) olarak teslim edilmesi gerekmektedir.</p>",
    "budget": 8500,
    "deadline": "2025-05-05T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Finansal Veri Analizi ve Risk Değerlendirme Sistemi",
    "description": "<p>Mikro kredi veren fintech girişimimiz için, başvuru yapan kullanıcıların kredi risk skorunu (Credit Scoring) hesaplayan otomatik bir karar destek sistemi geliştirmek istiyoruz. Geleneksel banka verileri yerine alternatif verileri kullanacağız.</p><p><strong>Analiz detayları:</strong></p><ul><li><strong>Veri Kaynakları:</strong> Kullanıcının GSM fatura ödeme düzeni, e-ticaret harcamaları ve uygulama içi davranış verileri API üzerinden toplanıp analiz edilecektir.</li><li><strong>Anomali Tespiti:</strong> Başvurularda sahtecilik (Fraud) riskini azaltmak için 'Isolation Forest' veya 'Autoencoder' yöntemleri kullanılarak normal dışı davranışlar tespit edilmelidir.</li><li><strong>Skorlama Kartı:</strong> Lojistik Regresyon veya Karar Ağaçları kullanılarak, her müşteriye 0 ile 1000 arasında şeffaf ve açıklanabilir bir risk puanı atayan algoritma yazılmalıdır.</li><li><strong>Modelin Açıklanabilirliği:</strong> Neden kredi verilmediğini müşteriye izah edebilmek için SHAP (Shapley Additive Explanations) değerleri kullanılarak modelin kararları yorumlanmalıdır.</li><li><strong>API Dağıtımı:</strong> Eğitilen model, Flask veya FastAPI ile bir mikroservis haline getirilmeli ve canlı sistemden gelen isteklere <200ms içinde yanıt vermelidir.</li></ul><p>Finansal regülasyonlara uyum için modelin yanlılık (Bias) içermediğinin test edilmesi önemlidir.</p>",
    "budget": 9200,
    "deadline": "2025-05-10T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Görüntü İşleme ve Nesne Tanıma Sistemi",
    "description": "<p>Fabrikamızdaki üretim bandında, hatalı üretilen parçaları (çatlak, boya hatası vb.) kameralar aracılığıyla otomatik tespit edecek bir Bilgisayarlı Görü (Computer Vision) projesi başlatıyoruz.</p><p><strong>Teknik gereksinimler:</strong></p><ul><li><strong>Veri Etiketleme:</strong> Tarafımızdan sağlanacak 5.000 adetlik parça fotoğrafı, LabelImg veya CVAT araçları kullanılarak 'Hatalı' ve 'Sağlam' olarak etiketlenmelidir.</li><li><strong>Model Mimarisi:</strong> Gerçek zamanlı tespit yapılması gerektiği için YOLOv8 veya MobileNet gibi hızlı ve hafif (Lightweight) modeller tercih edilmelidir.</li><li><strong>Veri Çoğaltma (Augmentation):</strong> Eğitim setini zenginleştirmek için görseller döndürülmeli, ışık ayarları değiştirilmeli ve gürültü eklenerek modelin genelleştirme yeteneği artırılmalıdır.</li><li><strong>Donanım Uyumu:</strong> Eğitilen modelin, üretim hattındaki NVIDIA Jetson veya Raspberry Pi gibi uç cihazlarda (Edge Computing) çalışabilecek şekilde optimize edilmesi (Quantization) gerekmektedir.</li><li><strong>Arayüz:</strong> Operatörün hatalı ürünleri ekranda kırmızı çerçeve içinde görebileceği basit bir izleme arayüzü (Streamlit veya OpenCV) kodlanmalıdır.</li></ul><p>Modelin üretim bandındaki başarı oranının (mAP) %95'in üzerinde olması hedeflenmektedir.</p>",
    "budget": 9500,
    "deadline": "2025-05-08T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Zaman Serisi Tahmin ve Talep Planlama Modeli",
    "description": "<p>Perakende zincirimizdeki 50 mağazanın stoklarını optimize etmek için, önümüzdeki 3 ayın ürün bazlı satış tahminlerini yapacak (Demand Forecasting) istatistiksel bir model arıyoruz.</p><p><strong>Proje kapsamı:</strong></p><ul><li><strong>Veri Analizi:</strong> Son 5 yılın satış verileri incelenerek; mevsimsellik (Seasonality), trendler ve döngüsel hareketler (Cycles) ayrıştırılmalıdır.</li><li><strong>Model Kıyaslama:</strong> Klasik ARIMA/SARIMA modelleri ile modern Prophet ve LSTM (Derin Öğrenme) modelleri kıyaslanarak en düşük hata payına (RMSE/MAPE) sahip olan seçilmelidir.</li><li><strong>Dış Faktörler:</strong> Modele sadece geçmiş satışlar değil; bayram günleri, hava durumu tahminleri ve rakip kampanyaları gibi dış değişkenler (Exogenous Variables) de eklenmelidir.</li><li><strong>Otomasyon:</strong> Model her hafta yeni gelen satış verileriyle kendini güncellemeli (Retraining) ve tahminleri ERP sistemine CSV formatında aktarmalıdır.</li><li><strong>Güven Aralığı:</strong> Tahminlerin kesin değer yerine bir aralık (Örn: 100-120 adet arası satar) olarak verilmesi stok planlaması için daha değerlidir.</li></ul><p>Raporlama aşamasında PowerBI veya Tableau entegrasyonu için veri hazırlığı da beklenmektedir.</p>",
    "budget": 7800,
    "deadline": "2025-04-28T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Doğal Dil İşleme ve Metin Analizi Platformu",
    "description": "<p>Müşteri hizmetlerine gelen binlerce e-posta ve şikayet kaydını manuel okumak yerine, yapay zeka ile otomatik sınıflandıran ve duygu analizi yapan bir NLP (Natural Language Processing) motoru geliştirmek istiyoruz.</p><p><strong>İşlevsel özellikler:</strong></p><ul><li><strong>Metin Ön İşleme:</strong> Türkçe metinler için Zemberek veya Turkish NLP kütüphaneleri kullanılarak kök bulma (Stemming), stop-words temizliği ve yazım hatası düzeltme işlemleri yapılmalıdır.</li><li><strong>Konu Modelleme:</strong> Gelen mesajların 'İade', 'Kargo', 'Ürün Kalitesi' gibi kategorilere otomatik ayrılması için BERT tabanlı (veya TF-IDF) sınıflandırma modelleri eğitilmelidir.</li><li><strong>Duygu Analizi:</strong> Mesajın tonunun Pozitif, Negatif veya Nötr olup olmadığını belirleyen ve acil müdahale gereken öfkeli müşterileri öne çıkaran bir algoritma kurulmalıdır.</li><li><strong>Varlık İsmi Tanıma (NER):</strong> Metin içindeki Sipariş Numarası, Telefon, Ad Soyad gibi bilgilerin otomatik çekilerek CRM sistemine işlenmesi sağlanmalıdır.</li><li><strong>Özetleme:</strong> Uzun şikayet maillerinin ana fikrini tek cümleyle özetleyen (Abstractive Summarization) bir model entegrasyonu yapılmalıdır.</li></ul><p>Hugging Face üzerindeki önceden eğitilmiş (Pre-trained) Türkçe modellerin fine-tune edilmesi süreci hızlandıracaktır.</p>",
    "budget": 8200,
    "deadline": "2025-05-01T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Öneri Sistemi ve Kişiselleştirme Motoru",
    "description": "<p>Video streaming platformumuzda kullanıcıların daha uzun süre vakit geçirmesini sağlamak için, 'Bunu izleyen şunları da izledi' mantığında çalışan hibrit bir öneri motoru (Recommendation Engine) kurmak istiyoruz.</p><p><strong>Algoritma detayları:</strong></p><ul><li><strong>İşbirlikçi Filtreleme:</strong> Benzer zevklere sahip kullanıcıların izleme geçmişine dayalı (User-based & Item-based Collaborative Filtering) Matris Faktörizasyon teknikleri uygulanmalıdır.</li><li><strong>İçerik Bazlı Filtreleme:</strong> Videoların türü, yönetmeni, oyuncuları ve açıklamaları analiz edilerek, kullanıcının sevdiği türlere yakın içerikler önerilmelidir.</li><li><strong>Soğuk Başlangıç (Cold Start):</strong> Yeni üye olan ve henüz verisi bulunmayan kullanıcılar için, popüler içerikler veya onboarding sırasında seçtiği ilgi alanlarına göre öneri sunan bir strateji geliştirilmelidir.</li><li><strong>Çeşitlilik:</strong> Sürekli aynı tarz filmleri önermek yerine (Filter Bubble), kullanıcının ilgisini çekebilecek farklı kategorilerden de serpiştirme yapan bir algoritma kurgulanmalıdır.</li><li><strong>A/B Testi:</strong> Geliştirilen modelin başarısını ölçmek için canlı trafikte küçük bir kitle üzerinde test edilerek 'İzleme Süresi' metrikleri karşılaştırılmalıdır.</li></ul><p>Sistem Python ile geliştirilmeli ve yüksek trafik altında hızlı yanıt verebilmesi için Redis ile desteklenmelidir.</p>",
    "budget": 8800,
    "deadline": "2025-05-03T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Sağlık Verisi Analizi ve Teşhis Destek Sistemi",
    "description": "<p>Özel bir hastane grubu için, anonimleştirilmiş hasta verilerini kullanarak diyabet ve kalp rahatsızlığı riskini erken aşamada tespit edebilecek bir veri bilimi projesi yürütüyoruz.</p><p><strong>Proje gereksinimleri:</strong></p><ul><li><strong>Veri Temizliği:</strong> Hastane veritabanındaki (HIS) tutarsız laboratuvar sonuçları ve eksik kayıtlar, istatistiksel yöntemlerle (Mean/Median Imputation) temizlenmelidir.</li><li><strong>Özellik Seçimi:</strong> Hastalığın teşhisinde en etkili olan parametrelerin (Örn: BMI, HbA1c, Yaş) belirlenmesi için Korelasyon Analizi ve Feature Importance çalışmaları yapılmalıdır.</li><li><strong>Tahmin Modeli:</strong> Lojistik Regresyon ve Destek Vektör Makineleri (SVM) kullanılarak, hastanın 'Yüksek Risk' veya 'Düşük Risk' grubunda olduğunu sınıflandıran modeller eğitilmelidir.</li><li><strong>Validasyon:</strong> Tıbbi bir proje olduğu için modelin başarısı sadece doğrulukla (Accuracy) değil; Hassasiyet (Sensitivity) ve Özgüllük (Specificity) metrikleriyle de kanıtlanmalıdır.</li><li><strong>Dashboard:</strong> Doktorların hasta verilerini girip risk skorunu görebileceği, grafiklerle desteklenmiş basit bir web arayüzü tasarlanmalıdır.</li></ul><p>Hasta gizliliği (HIPAA/KVKK) nedeniyle veriler üzerinde çalışılırken şifreleme ve anonimleştirme kurallarına sıkı sıkıya uyulmalıdır.</p>",
    "budget": 9800,
    "deadline": "2025-05-12T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Sosyal Medya Analitiği ve Trend Tespiti",
    "description": "<p>Bir reklam ajansı olarak, müşterilerimizin markaları hakkında sosyal medyada konuşulanları takip etmek ve kriz anlarını önceden sezmek için Twitter ve Instagram verilerini işleyen bir araç istiyoruz.</p><p><strong>Analiz kapsamı:</strong></p><ul><li><strong>Veri Madenciliği:</strong> Twitter API veya Selenium kullanılarak, belirli hashtag'ler ve marka isimleri geçen paylaşımların (Scraping) toplanması sağlanmalıdır.</li><li><strong>Trend Analizi:</strong> Zaman serisi analizi ile markanın konuşulma hacmindeki ani artışlar (Spike) tespit edilip, viral olma potansiyeli taşıyan konular raporlanmalıdır.</li><li><strong>Influencer Tespiti:</strong> Marka hakkında konuşan hesaplar arasından, takipçi sayısı ve etkileşim oranına göre 'Kanaat Önderleri'ni belirleyen bir ağ analizi (Graph Theory) yapılmalıdır.</li><li><strong>Kelime Bulutu:</strong> Marka ile birlikte en sık kullanılan kelimelerin ve emojilerin görselleştirildiği (Word Cloud) raporlar oluşturulmalıdır.</li><li><strong>Rakip Analizi:</strong> Rakip markaların sosyal medya performansı ile bizim markamızın kıyaslandığı (Benchmarking) karşılaştırmalı grafikler sunulmalıdır.</li></ul><p>Verilerin haftalık olarak otomatik çekilmesi ve PDF raporu olarak mail atılması otomasyonu da projeye dahildir.</p>",
    "budget": 6500,
    "deadline": "2025-04-25T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Enerji Tüketimi Tahmin ve Optimizasyon Modeli",
    "description": "<p>Büyük bir sanayi tesisinin elektrik maliyetlerini düşürmek amacıyla, akıllı sayaçlardan (IoT) gelen verileri analiz ederek enerji tüketimini optimize edecek bir yapay zeka çözümü arıyoruz.</p><p><strong>Çalışma detayları:</strong></p><ul><li><strong>Veri Entegrasyonu:</strong> Fabrikadaki sensörlerden gelen anlık voltaj, akım ve güç tüketim verilerinin InfluxDB gibi zaman serisi veritabanlarına kaydedilmesi.</li><li><strong>Patern Tanıma:</strong> Makinelerin boşta çalıştığı veya gereksiz enerji tükettiği zaman aralıklarının 'Kümeleme' (K-Means Clustering) algoritmalarıyla tespit edilmesi.</li><li><strong>Tepe Yük Tahmini:</strong> Elektrik fiyatlarının yüksek olduğu saatlerde (Peak Hours) tüketimin ne olacağını önceden tahmin edip, üretimi kaydırma önerileri sunan bir model.</li><li><strong>Anomali Uyarısı:</strong> Bir makinenin normalden fazla enerji çekmeye başlaması durumunda (Arıza habercisi olabilir), bakım ekibine uyarı gönderen sistem.</li><li><strong>Tasarruf Raporu:</strong> Modelin önerileri uygulandığında aylık ne kadar tasarruf edileceğini simüle eden bir senaryo analizi (What-If Analysis).</li></ul><p>Modelin Python ortamında geliştirilmesi ve sonuçların Grafana panosunda görselleştirilmesi beklenmektedir.</p>",
    "budget": 8900,
    "deadline": "2025-05-05T00:00:00.000Z",
    "status": "open"
  },
  {
    "title": "Veri Gölü ve Büyük Veri İşleme Altyapısı",
    "description": "<p>Şirketimizin verisi terabayt seviyelerine ulaştığı için klasik SQL sorguları artık yanıt vermiyor. Apache Spark kullanarak verilerimizi işleyecek ve bir Veri Gölü (Data Lake) mimarisi kuracak büyük veri mühendisi arıyoruz.</p><p><strong>Altyapı bileşenleri:</strong></p><ul><li><strong>Depolama Katmanı:</strong> Tüm ham verilerin (Loglar, CSV, JSON) AWS S3 veya HDFS üzerinde 'Parquet' formatında, sıkıştırılmış ve bölümlenmiş (Partitioned) olarak saklanması.</li><li><strong>Veri Kataloğu:</strong> Hangi verinin nerede olduğunu ve şemasını takip etmek için AWS Glue veya Hive Metastore kurulumunun yapılması.</li><li><strong>Batch Processing:</strong> Günlük büyük veri işleme görevlerinin (Aggregation) PySpark ile yazılarak gece çalışacak şekilde Airflow ile zamanlanması.</li><li><strong>Stream Processing:</strong> Kafka'dan akan canlı verilerin Spark Streaming ile işlenerek anlık dashboard'lara beslenmesi.</li><li><strong>SQL Arayüzü:</strong> İş analistlerinin veri gölü üzerinde standart SQL sorguları çalıştırabilmesi için Athena veya Presto (Trino) konfigürasyonunun yapılması.</li></ul><p>Sistemin ölçeklenebilir olması ve işlem maliyetlerinin (Cluster boyutu) optimize edilmesi kritik başarı faktörüdür.</p>",
    "budget": 9900,
    "deadline": "2025-05-10T00:00:00.000Z",
    "status": "open"
  }
]